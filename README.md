This project focuses on detecting terrorism-related content in tweets using the Bidirectional Encoder Representations from Transformers (BERT) model. The objective is to classify tweets into categories such as safe text, offensive, and supports terrorism by leveraging BERTâ€™s contextual language understanding. The dataset used in this project was collected from Twitter and stored in a JSON file named `terrorism.json`. Each tweet was labeled according to its content, and preprocessing steps such as text cleaning, removal of URLs, mentions, hashtags, and emojis, conversion to lowercase, and tokenization using the BERT tokenizer were performed to prepare the data for model training.

The model architecture is based on the pre-trained `bert-base-uncased` transformer from Hugging Face, fine-tuned for multi-class text classification. After obtaining embeddings from the BERT encoder, a dropout layer and a dense layer with a softmax activation function were used to predict the class of each tweet. The model was trained using the AdamW optimizer and categorical cross-entropy loss function with a batch size of 16 for 3 to 5 epochs. Evaluation metrics included accuracy, precision, recall, and F1-score.

The fine-tuned model achieved an accuracy of approximately 93.4%, with strong precision and recall values, indicating its robustness in distinguishing between safe and terrorism-related tweets. The results demonstrate that BERT significantly outperforms traditional NLP approaches such as Logistic Regression and SVM, especially in understanding the context and semantics of online text.

This project was developed using Python, TensorFlow/PyTorch, Hugging Face Transformers, Scikit-learn, Pandas, and Matplotlib. Future enhancements may include expanding the dataset to multilingual tweets using multilingual BERT (mBERT), deploying the model as a web service via Flask or FastAPI, integrating real-time Twitter API streams, and applying explainable AI techniques like LIME or SHAP for interpretability.
