# -*- coding: utf-8 -*-
"""CAPFIN2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iHfz1gyOMG3rvZ5tWxGLaM9XMJojfCXJ

## **Importing** **Libraries**
"""

from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader

# Load labeled and new datasets
labeled_data = pd.read_csv("/content/tweets_1500.csv")
new_data = pd.read_csv("/content/lokesh_dataset.csv", encoding='ISO-8859-1')

# Rename columns for consistency
labeled_data.columns = ['tweet', 'label']
new_data.columns = ['tweet']

class TweetDataset(Dataset):
    def __init__(self, tweets, labels=None, tokenizer=None, max_len=128):  #initialising the dataset
        self.tweets = tweets
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self): #defining the dataset length
        return len(self.tweets)

    def __getitem__(self, idx): #fetching a single data sample
        tweet = str(self.tweets[idx]) #preparing the tweet for BERT
        inputs = self.tokenizer(
            tweet, max_length=self.max_len, truncation=True, padding="max_length", return_tensors="pt"
        )
        item = {key: val.squeeze(0) for key, val in inputs.items()}
        if self.labels is not None:
            item["labels"] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

"""## **Dataset Spliting into test and train**"""

tweets = labeled_data["tweet"].tolist()
labels = labeled_data["label"].tolist()

train_tweets, val_tweets, train_labels, val_labels = train_test_split(tweets, labels, test_size=0.2)

train_dataset = TweetDataset(train_tweets, train_labels, tokenizer)
val_dataset = TweetDataset(val_tweets, val_labels, tokenizer)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = torch.argmax(torch.tensor(logits), dim=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average="binary")
    acc = accuracy_score(labels, predictions)
    return {"accuracy": acc, "f1": f1, "precision": precision, "recall": recall}

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
)

"""## **Training the model for Labeling Dataset**"""

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
)

# Train the model
trainer.train()

new_tweets = new_data["tweet"].tolist()
new_dataset = TweetDataset(new_tweets, tokenizer=tokenizer)
new_loader = DataLoader(new_dataset, batch_size=16)

# Perform predictions
model.eval()
predictions = []
with torch.no_grad():
    for batch in new_loader:
        inputs = {key: val.to(model.device) for key, val in batch.items() if key != "labels"}
        outputs = model(**inputs)
        logits = outputs.logits
        batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()
        predictions.extend(batch_predictions)

"""## **Saving Labeled Dataset**"""

# Add predictions to the new dataset and save
new_data["label"] = predictions
new_data.to_csv("labeled_new_dataset.csv", index=False)

print("New dataset labeled and saved as 'labeled_new_dataset.csv'.")

"""## Starting With **New Labeled Dataset**

# **Importing required Libraries**
"""

import pandas as pd
import re
import torch
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from torch.utils.data import Dataset

# Set device for computation
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

"""# **Loading and Preprocessing**"""

# Load labeled dataset
data = pd.read_csv("/content/labeled_new_dataset.csv")

# Define preprocessing function for text
def preprocess_text(text):
    text = re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"#\w+", "", text)
    text = re.sub(r"[^\w\s]", "", text)
    text = text.lower().strip()
    return text

# Apply preprocessing to tweets
data["tweet"] = data["tweet"].apply(preprocess_text)

# Preprocess and clean up labels
print("Unique labels before cleanup:", data["label"].unique())
data["label"] = pd.to_numeric(data["label"], errors="coerce")
data = data.dropna(subset=["label"])
data["label"] = data["label"].astype(int)
print("Unique labels after cleanup:", data["label"].unique())

# Split data into training and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(
    data["tweet"], data["label"], test_size=0.2, random_state=42
)

# Print dataset sizes for PyTorch Dataset
print(f"Training dataset size: {len(train_texts)}")
print(f"Validation dataset size: {len(val_texts)}")

class TweetDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts.reset_index(drop=True)
        self.labels = labels.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts.iloc[idx]
        label = int(self.labels.iloc[idx])
        inputs = self.tokenizer(
            text, max_length=self.max_len, truncation=True, padding="max_length", return_tensors="pt"
        )
        item = {key: val.squeeze(0) for key, val in inputs.items()}
        item["labels"] = torch.tensor(label, dtype=torch.long)
        return item

# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Prepare datasets
train_dataset = TweetDataset(train_texts, train_labels, tokenizer)
val_dataset = TweetDataset(val_texts, val_labels, tokenizer)

# Load pre-trained BERT model for classification
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
model.to(device)

# Set training arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_steps=10,
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = torch.argmax(torch.tensor(logits), dim=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average="binary")
    acc = accuracy_score(labels, predictions)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

# Train the model
trainer.train()

# Evaluate the model
metrics = trainer.evaluate()

# Print evaluation metrics
print("\nEvaluation Metrics:")
for key, value in metrics.items():
    print(f"{key}: {value:.4f}")

from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Get predictions and labels
predictions = trainer.predict(val_dataset)
pred_logits = predictions.predictions
pred_labels = np.argmax(pred_logits, axis=1)

# Calculate confusion matrix
conf_matrix = confusion_matrix(val_labels, pred_labels)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(val_labels), yticklabels=np.unique(val_labels))
plt.title("Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Detailed classification report
print("\nClassification Report:")
print(classification_report(val_labels, pred_labels))

# Plot ROC curve (for binary classification only)
if len(np.unique(val_labels)) == 2:
    y_true_binary = (val_labels == np.unique(val_labels)[1]).astype(int)
    y_scores = pred_logits[:, 1]

    fpr, tpr, thresholds = roc_curve(y_true_binary, y_scores)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f"ROC curve (AUC = {roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Receiver Operating Characteristic (ROC) Curve")
    plt.legend(loc="lower right")
    plt.show()
else:
    print("ROC curve is only applicable for binary classification.")

import pandas as pd
import re
import torch
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from torch.utils.data import Dataset
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import random
import nltk
nltk.download('punkt_tab')
# Download NLTK resources
nltk.download("stopwords")
nltk.download("punkt")
nltk.download("wordnet")

"""## **WITH DATA AUGUMENTATION**"""

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load dataset
data = pd.read_csv("/content/labeled_new_dataset.csv")

# Preprocessing function
def preprocess_text(text):
    text = re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"#\w+", "", text)
    text = re.sub(r"[^\w\s]", "", text)
    text = text.lower().strip()

    # Tokenize and remove stop words
    stop_words = set(stopwords.words("english"))
    words = word_tokenize(text)
    text = " ".join(word for word in words if word not in stop_words)

    return text

# Data augmentation
def augment_text(text):
    augmentation_methods = [
        synonym_replacement,
        random_deletion
    ]
    # Choose a random augmentation
    method = random.choice(augmentation_methods)
    return method(text)
def synonym_replacement(text):
    words = word_tokenize(text)
    synonyms = []
    for word in words:
        synsets = nltk.corpus.wordnet.synsets(word)
        if synsets:
            synonyms.append(synsets[0].lemmas()[0].name())
        else:
            synonyms.append(word)
    return " ".join(synonyms)

def random_deletion(text, p=0.05):
    words = word_tokenize(text)
    if len(words) == 1:
        return text
    filtered = [word for word in words if random.uniform(0, 1) > p]
    return " ".join(filtered) if filtered else text

# Apply preprocessing
data["tweet"] = data["tweet"].apply(preprocess_text)

# Augment data
augmented_data = data.copy()
augmented_data["tweet"] = augmented_data["tweet"].apply(augment_text)

# Combine original and augmented data
data = pd.concat([data, augmented_data]).reset_index(drop=True)

# Preprocessing labels
print("Unique labels before cleanup:", data["label"].unique())
data["label"] = pd.to_numeric(data["label"], errors="coerce")
data = data.dropna(subset=["label"])
data["label"] = data["label"].astype(int)
print("Unique labels after cleanup:", data["label"].unique())

# Train-test split
train_texts, val_texts, train_labels, val_labels = train_test_split(
    data["tweet"], data["label"], test_size=0.2, random_state=42
)

# Tokenization and Dataset preparation
class TweetDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts.reset_index(drop=True)
        self.labels = labels.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts.iloc[idx]
        label = int(self.labels.iloc[idx])
        inputs = self.tokenizer(
            text, max_length=self.max_len, truncation=True, padding="max_length", return_tensors="pt"
        )
        item = {key: val.squeeze(0) for key, val in inputs.items()}
        item["labels"] = torch.tensor(label, dtype=torch.long)
        return item

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

train_dataset = TweetDataset(train_texts, train_labels, tokenizer)
val_dataset = TweetDataset(val_texts, val_labels, tokenizer)

# Model and Training setup
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
model.to(device)

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_steps=10,
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = torch.argmax(torch.tensor(logits), dim=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average="binary")
    acc = accuracy_score(labels, predictions)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

# Train model
trainer.train()

# Evaluate model
metrics = trainer.evaluate()
print("\nEvaluation Metrics:")
for key, value in metrics.items():
    print(f"{key}: {value:.4f}")

from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Get predictions and labels
predictions = trainer.predict(val_dataset)
pred_logits = predictions.predictions
pred_labels = np.argmax(pred_logits, axis=1)

# Calculate confusion matrix
conf_matrix = confusion_matrix(val_labels, pred_labels)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(val_labels), yticklabels=np.unique(val_labels))
plt.title("Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Detailed classification report
print("\nClassification Report:")
print(classification_report(val_labels, pred_labels))

# Plot ROC curve (for binary classification only)
if len(np.unique(val_labels)) == 2:
    y_true_binary = (val_labels == np.unique(val_labels)[1]).astype(int)
    y_scores = pred_logits[:, 1]

    fpr, tpr, thresholds = roc_curve(y_true_binary, y_scores)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f"ROC curve (AUC = {roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Receiver Operating Characteristic (ROC) Curve")
    plt.legend(loc="lower right")
    plt.show()
else:
    print("ROC curve is only applicable for binary classification.")